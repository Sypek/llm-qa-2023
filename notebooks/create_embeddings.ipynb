{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain --quiet\n",
    "!pip install unstructured --quiet\n",
    "!pip install \"unstructured[md]\" --quiet\n",
    "!pip install chromadb --quiet\n",
    "!pip install -U boto3 --quiet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader, DirectoryLoader, S3DirectoryLoader\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STACK = 'LLMStack'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cf_config(stackname: str):\n",
    "    cf_client = boto3.client('cloudformation')\n",
    "\n",
    "    response = cf_client.describe_stacks(StackName=stackname)\n",
    "    outputs = response[\"Stacks\"][0][\"Outputs\"]\n",
    "\n",
    "    cf_outputs = {}\n",
    "    for i in outputs:\n",
    "        cf_outputs[i['OutputKey']] = i['OutputValue']\n",
    "    return cf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_config = get_cf_config(STACK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files_from_s3(bucket_name, dir_name: str = 'input_data'):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "        \n",
    "    s3_client = boto3.client('s3')\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    objs = bucket.objects.filter(Prefix=dir_name)\n",
    "    for obj in objs:\n",
    "        s3_client.download_file(bucket_name, obj.key, obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_files_from_s3(stack_config['S3BucketName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('input_data',\n",
    "                         loader_cls=UnstructuredMarkdownLoader,\n",
    "                         show_progress=True,\n",
    "                         use_multithreading=True)\n",
    "\n",
    "docs = loader.load()\n",
    "print(f'Lazy loaded {len(docs)} docs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_splitter = MarkdownTextSplitter()\n",
    "texts = md_splitter.split_documents(docs)\n",
    "print(f'There are {len(texts)} texts after splitting.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 500\n",
    "chunk_overlap = 100\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Split\n",
    "splits = text_splitter.split_documents(texts)\n",
    "print(f'There are {len(splits)} texts after splitting.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, List\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"text_inputs\": inputs, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> List[List[float]]:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_handler = ContentHandler()\n",
    "\n",
    "embeddings = SagemakerEndpointEmbeddings( \n",
    "    endpoint_name=stack_config['SageMakerEndpointEmbeddings'],\n",
    "    region_name=stack_config['AWSRegion'],\n",
    "    content_handler=content_handler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "os.makedirs('chroma')\n",
    "chroma_client = chromadb.PersistentClient(path=\"chroma\")\n",
    "collection = chroma_client.create_collection(name=\"sagemaker_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading is slow due to inferencing one doc at a time.\n",
    "# Required some inference in chunks to speed up\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "collection_input_embeddings = []\n",
    "collection_input_documents = []\n",
    "collection_input_metadatas = []\n",
    "collection_input_ids = []\n",
    "errors = []\n",
    "\n",
    "for idx, text in tqdm(enumerate(splits), total=len(splits)):\n",
    "    try:\n",
    "        page_content = text.page_content\n",
    "        metadata = text.metadata\n",
    "        idx_str = f'id{idx}'\n",
    "        emb = embeddings.embed_query(page_content)\n",
    "\n",
    "        collection_input_embeddings.append(emb)\n",
    "        collection_input_documents.append(page_content)\n",
    "        collection_input_metadatas.append(metadata)\n",
    "        collection_input_ids.append(idx_str)\n",
    "    except Exception as e:\n",
    "        error = {\n",
    "            'metadata': metadata,\n",
    "            'page_content': page_content,\n",
    "            'error': e\n",
    "        }\n",
    "        errors.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    embeddings=collection_input_embeddings,\n",
    "    documents=collection_input_documents,\n",
    "    metadatas=collection_input_metadatas,\n",
    "    ids=collection_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'llmstacks3-s3bucket-1b39zw19b74me'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "def upload_to_s3(local_file, s3_bucket, s3_object_name):\n",
    "    s3.upload_file(local_file, s3_bucket, s3_object_name)\n",
    "    print(f\"Uploaded {local_file} to {s3_bucket}/{s3_object_name}\")\n",
    "\n",
    "for root, dirs, files in os.walk('chroma'):\n",
    "    for file in files:\n",
    "        local_path = os.path.join(root, file)\n",
    "        upload_to_s3(local_path, bucket_name, local_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
