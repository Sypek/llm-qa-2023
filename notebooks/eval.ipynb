{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "You are a smart assistant designed to come up with meaninful question and answer pair. The question should be to the point and the answer should be as detailed as possible.\n",
    "Given a piece of text, you must come up with a question and answer pair that can be used to evaluate a QA bot. Do not make up stuff. Stick to the text to come up with the question and answer pair.\n",
    "When coming up with this question/answer pair, you must respond in the following format:\n",
    "\n",
    "Please come up with a question/answer pair, in the specified JSON format, for the following text:\n",
    "{text}\n",
    "\n",
    "Now write a question and then the answer to that question.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path = '../data/sagemaker_documentation/amazon-sagemaker-toolkits.md'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lazy loaded 1 docs.\n"
     ]
    }
   ],
   "source": [
    "loader = UnstructuredMarkdownLoader(file_path=doc_path)\n",
    "docs = loader.load()\n",
    "print(f'Lazy loaded {len(docs)} docs.')\n",
    "doc = docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=['text'],\n",
    "    template=query,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  ../data/models/llama-2-7b-chat.ggmlv3.q4_0.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: using Metal\n",
      "llama.cpp: loading model from ../data/models/llama-2-7b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 4013.73 MB (+ 1024.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1024.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/kubasyp/.pyenv/versions/3.9.14/envs/llm-qa/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x2833526e0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x283355450\n",
      "ggml_metal_init: loaded kernel_mul                            0x283362f80\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x2833631e0\n",
      "ggml_metal_init: loaded kernel_scale                          0x283363440\n",
      "ggml_metal_init: loaded kernel_silu                           0x2833636a0\n",
      "ggml_metal_init: loaded kernel_relu                           0x283363900\n",
      "ggml_metal_init: loaded kernel_gelu                           0x283363b60\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x283363dc0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x283364020\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x283364280\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x2833644e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x283364740\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x2833649a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x283364dd0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x283367ca0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x283367f00\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x283368160\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x2833683c0\n",
      "ggml_metal_init: loaded kernel_norm                           0x283368620\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x283368880\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x283368ae0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x283368d40\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x283368fa0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x283369200\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x283369460\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x2833696c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x283369920\n",
      "ggml_metal_init: loaded kernel_rope                           0x283369b80\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x283369de0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x28336a040\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x28336a2a0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x28336a500\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =    70.31 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3616.08 MB, ( 8658.02 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.17 MB, ( 8668.19 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1026.00 MB, ( 9694.19 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   228.00 MB, ( 9922.19 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, (10082.19 / 21845.34)\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = '../data/models/llama-2-7b-chat.ggmlv3.q4_0.bin'\n",
    "\n",
    "\n",
    "llm = GPT4All(\n",
    "    model=MODEL_PATH,\n",
    "    max_tokens=2048,\n",
    "    temp=0\n",
    ")\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the chain only specifying the input variable.\n",
    "resutl = chain.run(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the structure of the file folder created by SageMaker when training a model?\n",
      "\n",
      "Answer: When SageMaker trains a model, it creates the following file folder structure in the container's /opt/ml directory:\n",
      "\n",
      "/opt/ml\n",
      "├── input\n",
      "│   ├── config\n",
      "│   │   ├── hyperparameters.json\n",
      "│   │   └── resourceConfig.json\n",
      "│   └── data\n",
      "│       └── <channel_name>\n",
      "│           └── <input data>\n",
      "├── model\n",
      "│\n",
      "├── code\n",
      "│\n",
      "├── output\n",
      "│\n",
      "└── failure\n",
      "\n",
      "The /opt/ml directory contains the files and folders created by SageMaker during the training process. The input, config, data, model, and output directories contain the files and subdirectories used for each step of the training process.\n"
     ]
    }
   ],
   "source": [
    "print(resutl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
